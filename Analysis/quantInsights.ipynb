{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory = \"Dataset/ijcnlp_dailydialog\"\n",
    "textFile = dataDirectory + \"/dialogues_text.txt\"\n",
    "emotionFile = dataDirectory + \"/dialogues_emotion.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLinebyLine(filePath,maxLine=float('inf')):\n",
    "    currLine=0\n",
    "    with open(filePath, 'r') as file:\n",
    "    # Loop through each line in the file\n",
    "        for line in file:\n",
    "            if(currLine==maxLine):\n",
    "                return\n",
    "            # Process each line here\n",
    "            print(line.strip())  # Print the line after stripping whitespace (e.g., newline characters)\n",
    "            currLine+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEmotionConversations(emotionFile, textFile, label):\n",
    "    conversationNumbers = set()\n",
    "    lineNumber = 0\n",
    "    count=0\n",
    "    with open(emotionFile, 'r') as file:\n",
    "    # Loop through each line in the file\n",
    "        for line in file:\n",
    "\n",
    "            if(label in line):\n",
    "                conversationNumbers.add(lineNumber)\n",
    "                count+=line.count(label)\n",
    "            lineNumber+=1\n",
    "    #print(len(conversationNumbers), count)\n",
    "    return conversationNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attachLabels(line):\n",
    "    rawConvo=line.split('__eou__')\n",
    "    proConvo = [item for item in rawConvo if item != '\\n']\n",
    "    for i in range(len(proConvo)):\n",
    "        if (i%2 == 1):\n",
    "            proConvo[i] = \"Person 2: \" + proConvo[i]\n",
    "        else:\n",
    "            proConvo[i] = \"Person 1: \" + proConvo[i]\n",
    "    return proConvo\n",
    "\n",
    "def prepareString(withLabels,currentIndex,history):\n",
    "    currString = ''\n",
    "    if(history == 0):\n",
    "        return withLabels[currentIndex]\n",
    "    elif(history ==1):\n",
    "        if(currentIndex==0):\n",
    "            return withLabels[0]\n",
    "        else:\n",
    "            currString += withLabels[currentIndex-1] + '\\n'\n",
    "            currString += withLabels[currentIndex] + '\\n'\n",
    "            return currString\n",
    "    elif history == 2:\n",
    "        for i in range(0,currentIndex+1):\n",
    "            currString = currString + withLabels[i] + \"'\\n\"\n",
    "        return currString\n",
    "    else:\n",
    "        raise ValueError(\"Invalid History:\", history)\n",
    "\n",
    "def preparePrompt(stringGPT, context,currLine,message):\n",
    "    finalPrompt = stringGPT\n",
    "    if(context):\n",
    "        finalPrompt = \"The topic of the following conversation is \" + getContext(currLine)+\".\\n\" + finalPrompt\n",
    "    else:\n",
    "        finalPrompt = \"Here is a conversation snippet:\" +\"\\n\" + finalPrompt\n",
    "\n",
    "    if ('@rukhshan@' in finalPrompt):\n",
    "        finalPrompt = finalPrompt.replace(\"@rukhshan@\",message)\n",
    "        #print(\"Final Prompt\", finalPrompt)\n",
    "    return finalPrompt\n",
    "\n",
    "def callGPT(finalPrompt):\n",
    "    #openai.api_type = \"azure\"\n",
    "    #print(\"Prompt:\\n\", finalPrompt,'\\n\\n')\n",
    "    openai.api_base = \"REMOVED\"\n",
    "    openai.api_key = 'REMOVED'\n",
    "    openai.api_version = \"2023-05-15\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"GPT4\",\n",
    "        messages=[\n",
    "\n",
    "            {\"role\": \"user\", \"content\": finalPrompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if (\"N\" in response['choices'][0]['message']['content']):\n",
    "        return '0'\n",
    "    elif(\"Y\" in response['choices'][0]['message']['content']):\n",
    "        return '1'\n",
    "    elif(\"M\" in response['choices'][0]['message']['content']):\n",
    "        return '2'\n",
    "    else:\n",
    "        print(\"K Produced\",response['choices'][0]['message']['content'], \"\\nPrompt:\", finalPrompt)\n",
    "        return 'K'\n",
    "    \n",
    "def writeOutput(outputGPT, promptChosen,writeFilePath):\n",
    "\n",
    "    finalOutput = promptChosen.replace(\"\\n\",\"\") + \"\\n\" + outputGPT\n",
    "\n",
    "    # Open the file in write mode ('w')\n",
    "    # This will create the file if it doesn't exist or completely overwrite it if it does\n",
    "    with open(writeFilePath, 'w') as file:\n",
    "        file.write(finalOutput)\n",
    "\n",
    "    # The file is automatically closed when the 'with' block exits\n",
    "\n",
    "def getContext(currLineIndex):\n",
    "    with open(\"/Users/rukhshanharoon/Desktop/LLMChattingApp/Analysis/Dataset/ijcnlp_dailydialog/dialogues_topic.txt\", \"r\") as file:\n",
    "        # Read all lines into a list\n",
    "        lines = file.readlines()\n",
    "    contextNumber = lines[currLineIndex].replace(\"\\n\",\"\")\n",
    "    if(contextNumber==\"1\"):\n",
    "        return \"Ordinary Life\"\n",
    "    elif(contextNumber==\"2\"):\n",
    "        return \"School Life\"\n",
    "    elif(contextNumber==\"3\"):\n",
    "        return \"Culture & Education\"\n",
    "    elif(contextNumber==\"4\"):\n",
    "        return \"Attitude & Emotion\"\n",
    "    elif(contextNumber==\"5\"):\n",
    "        return \"Relationship\"\n",
    "    elif(contextNumber==\"6\"):\n",
    "        return \"Tourism\"\n",
    "    elif(contextNumber==\"7\"):\n",
    "        return \"Health\"\n",
    "    elif(contextNumber==\"8\"):\n",
    "        return \"Work\"\n",
    "    elif(contextNumber==\"9\"):\n",
    "        return \"Politics\"\n",
    "    elif(contextNumber==\"10\"):\n",
    "        return \"Finance\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid contextNumber:\", contextNumber)\n",
    "    \n",
    "def readSpecificConversations(filePath,conversationNumbers,promptChosen,writeFilePath, history=2,context=False, max=float('inf'), startFrom=0):\n",
    "    currLine=0\n",
    "    currOutput=''\n",
    "    currCount=0\n",
    "    reqCount=0\n",
    "    with open(filePath, 'r') as file:\n",
    "    # Loop through each line in the file\n",
    "        for line in file:\n",
    "            if(currLine in conversationNumbers and currLine>=startFrom):\n",
    "                convoPrinted = False\n",
    "                withLabels = attachLabels(line)\n",
    "                for i in range(len(withLabels)):\n",
    "                    convoGPT = prepareString(withLabels,i,history)\n",
    "                    finalPrompt = preparePrompt(convoGPT + promptChosen,context,currLine, withLabels[i])\n",
    "                    outputGPT=\"\"\n",
    "                    GPTdone = False\n",
    "                    while(GPTdone==False):\n",
    "                        try:  \n",
    "                            outputGPT = callGPT(finalPrompt)\n",
    "                            reqCount+=1\n",
    "                            GPTdone=True\n",
    "                        except Exception as e:\n",
    "                            #prints index\n",
    "                            #line number is index + 1\n",
    "                            #print(\"Conversation index: \", currLine)\n",
    "                            if('response was filtered due to the prompt triggering' in str(e)):\n",
    "                                print(\"Skipped by GPT.\\n\" )    \n",
    "                                if(convoPrinted==False):\n",
    "                                    print(\"Convo:\\n\", convoGPT)\n",
    "                                    convoPrinted = True\n",
    "                                outputGPT = \"S\"\n",
    "                                GPTdone=True\n",
    "                            else:\n",
    "                                #print(\"Sleeping for 8.\")\n",
    "                                time.sleep(8)\n",
    "                    currOutput = currOutput +\" \" + outputGPT\n",
    "                #currOutput = currOutput + \"\\n\"\n",
    "                currCount+=1\n",
    "                if(currCount==max):\n",
    "                    writeOutput(currOutput, promptChosen,writeFilePath)\n",
    "                    print(\"Total Requests:\", reqCount)\n",
    "                    return '\\n'.join([line.lstrip() for line in currOutput.splitlines()])\n",
    "                else:\n",
    "                    currOutput = currOutput + \"\\n\"\n",
    "            \n",
    "            currLine+=1\n",
    "    writeOutput(currOutput, promptChosen,writeFilePath)\n",
    "    print(\"Total Requests:\", reqCount)\n",
    "    return '\\n'.join([line.lstrip() for line in currOutput.splitlines()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLabels(emotionFile, label, max=float('inf')):\n",
    "    conversations =\"\"\n",
    "    lineNumber = 0\n",
    "\n",
    "    with open(emotionFile, 'r') as file:\n",
    "    # Loop through each line in the file\n",
    "        for line in file:\n",
    "            if(lineNumber == max):\n",
    "                return conversations\n",
    "\n",
    "            if(label in line):\n",
    "                newLine=\"\"\n",
    "                for i in range (len(line)):\n",
    "                    if((line[i] != \" \") and (line[i] != \"\\n\") and (line[i] != \"1\")):\n",
    "                        newLine += '0'\n",
    "                    else:\n",
    "                        newLine +=line[i]\n",
    "                conversations += newLine\n",
    "                lineNumber+=1\n",
    "\n",
    "        \n",
    "    #print(len(conversationNumbers), count)\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeRealLabels(labels, name):\n",
    "    with open(name, 'w') as file:\n",
    "        file.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(realLabels, genLabelsName):\n",
    "\n",
    "    processedRealLabels=\"\"\n",
    "    processedGenLabels=\"\"\n",
    "    index=[]\n",
    "    selectedIndex=[]\n",
    "    indexCount=0\n",
    "    n=0\n",
    "    \n",
    "    \n",
    "    firstLine = False\n",
    "    with open(genLabelsName, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip first line\n",
    "            if(firstLine==False):\n",
    "                firstLine=True\n",
    "            elif 'S' not in line or 'K' not in line:\n",
    "                # If it doesn't contain \"S\", add the line to the processed content\n",
    "                cleanedLine = line.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "                processedGenLabels += cleanedLine\n",
    "                selectedIndex.append(indexCount)\n",
    "                indexCount+=1\n",
    "                n+=1\n",
    "                \n",
    "            else:\n",
    "                #indexes of lines that contain S in REAL file\n",
    "                index.append(indexCount)\n",
    "                indexCount+=1\n",
    "    \n",
    "    print(selectedIndex)\n",
    "    indexCount=0\n",
    "    lineCount=0\n",
    "    with open(realLabels, 'r') as file:\n",
    "        for line in file:\n",
    "            lineCount+=1\n",
    "            if ((indexCount in index) or not(indexCount in selectedIndex)):\n",
    "                indexCount+=1\n",
    "            else:\n",
    "                indexCount+=1\n",
    "                cleanedLine = line.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "                for char in cleanedLine:\n",
    "                    if(char =='1'):\n",
    "                        processedRealLabels += char\n",
    "                    else:\n",
    "                        processedRealLabels += '0'            \n",
    "    print(processedRealLabels)\n",
    "\n",
    "    return processedRealLabels, processedGenLabels,n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpretng Precision\n",
    "#Precision is a measure of true positives / true positives + false positives\n",
    "#Shows how many positives are true from the set of all elements marked as positive by the model\n",
    "#TP/(TP+FP)\n",
    "#5/(5+5) -> 0.5 means that out of all elements it marked as positive, only half were actually positive and the rest were negative. the higher the value, higher the precision\n",
    "#higher the precision, higher the quality of positive predictions\n",
    "\n",
    "#Interpreting Recall\n",
    "#shows the proportion of all true positives that the model was able to classify as positive \n",
    "#TP/(TP + FN)\n",
    "#5/(5+5) -> shows that it was only able to detect 5 out of the 10 true positive elements.\n",
    "#higher the recall, higher the sensitivity\n",
    "def PRFA(angryOnlyRealLabels, angryOnlyGenLabels,n, promptNumber, ITConvos, Prompt, Context):\n",
    "    # Initialize counters for True Positives (TP), False Positives (FP), True Negatives (TN), False Negatives (FN)\n",
    "    TP = FP = TN = FN = 0\n",
    "    '''\n",
    "    # Calculate TP, FP, TN, FN\n",
    "    for true_label, predicted_label in zip(angryOnlyRealLabels, angryOnlyGenLabels):\n",
    "        if (true_label == '1' and (predicted_label == '1' or predicted_label == \"2\")):\n",
    "            TP += 1\n",
    "        elif true_label == '0' and predicted_label == '1':\n",
    "            FP += 1\n",
    "        elif true_label == '0' and (predicted_label == '0' or predicted_label== \"2\"):\n",
    "            TN += 1\n",
    "        elif true_label == '1' and predicted_label == '0':\n",
    "            FN += 1\n",
    "    '''\n",
    "    for true_label, predicted_label in zip(angryOnlyRealLabels, angryOnlyGenLabels):\n",
    "        if (true_label == '1' and predicted_label == '1'):\n",
    "            TP += 1\n",
    "        elif true_label == '0' and predicted_label == '1':\n",
    "            FP += 1\n",
    "        elif true_label == '0' and predicted_label == '0':\n",
    "            TN += 1\n",
    "        elif true_label == '1' and predicted_label == '0':\n",
    "            FN += 1\n",
    "    \n",
    "    # Calculate Precision, Recall, F1 Score, and Accuracy\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    dataset = {\n",
    "    \"#Prompt\":,\n",
    "    \"#ITConvos\":,\n",
    "    \"Prompt\":,\n",
    "    \"Context\":,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1\": f1_score,\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"FP\": FP,\n",
    "    \"FN\": FN,\n",
    "    \"TP\": TP,\n",
    "    \"TN\": TN,\n",
    "    \"#FTMessages\": FP + FN + TP + TN,\n",
    "    \"#FTConvos\": n1\n",
    "}\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1_score)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"FP\", FP)\n",
    "    print(\"FN\", FN)\n",
    "    print(\"TP\",TP)\n",
    "    print(\"TN\",TN)\n",
    "    print(\"Total:\", FP+FN+TP+TN)\n",
    "    print(\"Total Convos:\",n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation indexes with at least 1 angry emotion:\n",
      " {1, 2, 10251, 12301, 15, 32, 2084, 4132, 4134, 10281, 4143, 12337, 2100, 2101, 2102, 2106, 2111, 2113, 4161, 4165, 6214, 76, 2126, 4179, 4182, 8278, 8283, 2140, 4188, 95, 96, 97, 2148, 4196, 12395, 4205, 4208, 4211, 12403, 2168, 4216, 4217, 12408, 4220, 8318, 8321, 4227, 4228, 8324, 4231, 136, 12425, 4237, 4243, 10388, 10389, 10390, 10392, 10393, 155, 4251, 157, 10395, 10396, 4256, 4257, 4259, 4263, 6139, 2218, 10413, 12653, 2223, 2224, 4273, 178, 10415, 2228, 10416, 2230, 4278, 4280, 186, 4283, 2236, 2237, 10429, 2239, 2241, 2243, 4308, 2262, 4310, 4314, 222, 4320, 4321, 2275, 4327, 2280, 8428, 2285, 12530, 4348, 2302, 4354, 8452, 4362, 2324, 10517, 4377, 4379, 4387, 295, 296, 6440, 8487, 300, 4398, 306, 311, 8503, 4414, 10564, 2374, 327, 4423, 8521, 4426, 4427, 2380, 8522, 8526, 2383, 2384, 4431, 338, 2387, 8534, 2391, 351, 2402, 4451, 12642, 357, 364, 2413, 4460, 2415, 368, 2416, 370, 4465, 12284, 12652, 4470, 12656, 4472, 2427, 380, 4477, 12674, 2435, 12677, 12678, 12679, 392, 12680, 12681, 4491, 4495, 2451, 4500, 4502, 408, 8600, 4506, 4512, 417, 4515, 2471, 4521, 2474, 2476, 2478, 4526, 4529, 4532, 4533, 4534, 2489, 6588, 4544, 6593, 8645, 4550, 2505, 4553, 6601, 2509, 2510, 6605, 8656, 4562, 2515, 2518, 4567, 6615, 2523, 2524, 2525, 2526, 4576, 4580, 4581, 2534, 4582, 4583, 10727, 490, 4590, 4592, 4596, 8692, 4598, 6652, 4606, 519, 4615, 4616, 6666, 8715, 4621, 4624, 529, 4625, 8726, 4633, 6684, 8738, 4648, 4662, 4663, 2617, 576, 4674, 4680, 6732, 8792, 2651, 2654, 2657, 2661, 6757, 4719, 4720, 4722, 6776, 636, 2687, 4743, 8840, 8843, 10895, 8849, 4756, 8853, 4759, 4760, 4761, 4762, 10908, 8864, 4769, 4770, 2728, 4776, 4777, 4778, 4780, 4782, 4783, 4787, 4794, 10938, 4800, 4801, 2758, 4816, 13017, 8922, 4829, 2785, 8936, 8939, 2811, 2816, 769, 8963, 6922, 789, 13088, 814, 9019, 2887, 9031, 9033, 4949, 2902, 4951, 2924, 2937, 4993, 11151, 7059, 917, 935, 2992, 3006, 7105, 9154, 9158, 967, 3017, 9166, 3023, 9168, 9170, 3030, 9179, 991, 3042, 3050, 11256, 3073, 3077, 3078, 3084, 3085, 3087, 1042, 3091, 3092, 3095, 3100, 3102, 3105, 3106, 3109, 3111, 5159, 3113, 3116, 3118, 3120, 5185, 5195, 3156, 3157, 5216, 5228, 9328, 9332, 9334, 3215, 1176, 9376, 1194, 3249, 3256, 3259, 7356, 5310, 3263, 3266, 9414, 11463, 3286, 1245, 5357, 3323, 3327, 1301, 9502, 1319, 5431, 9538, 1347, 1348, 11597, 1364, 11605, 1377, 7521, 5477, 3445, 11637, 11646, 11647, 11661, 1423, 11666, 1428, 1429, 11672, 1443, 3493, 3495, 3499, 7596, 3504, 11700, 1483, 7629, 7637, 1494, 7639, 1504, 1525, 1530, 3579, 5628, 3582, 1535, 3586, 1539, 1542, 1543, 7686, 11783, 9739, 1595, 11837, 1598, 11851, 11854, 11856, 11858, 11863, 11865, 1654, 9858, 1668, 11909, 9867, 9874, 9877, 5794, 5795, 7844, 7852, 3758, 5809, 3764, 3769, 7866, 3775, 7875, 9927, 1741, 5843, 1752, 1779, 12020, 1791, 5889, 1799, 9994, 9999, 3862, 12067, 1828, 5936, 12083, 5958, 12105, 12106, 5971, 5982, 10092, 1922, 12163, 10121, 10122, 10124, 10127, 1940, 12191, 10566, 12206, 1969, 1977, 8123, 10175, 12235, 10190, 1999, 12240, 6107, 12617, 2026, 2029, 10224, 2033, 12280, 12282, 2043, 6140}\n"
     ]
    }
   ],
   "source": [
    "#conversations with at least one message labeled as angry\n",
    "angryConversations = extractEmotionConversations(emotionFile,textFile,'1')\n",
    "print(\"Conversation indexes with at least 1 angry emotion:\\n\",angryConversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = extractLabels(emotionFile, '1', 40)\n",
    "writeRealLabels(labels, \"angryReal40.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    '\\nState if the tone/intent of the last message in the conversation above is negative or not this format: \"Y\" for yes and \"N\" for no.',\n",
    "    '\\nState if the tone/intent of the last message in the conversation above is negative (even if it is slightly negative) or not. Format output like this: \"Y\" for yes and \"N\" for no.',\n",
    "    '\\nState if the tone/intent of the last message in the conversation above is negative (only those which are extremely negative) or not. Format output like this: \"Y\" for yes and \"N\" for no.',\n",
    "    '\\nState if the tone/intent of the last message in the conversation above is extremely negative, clearly not negative, or maybe negative. Format output like this: \"Y\" for Extremely Negative and \"N\" for Clearly Not Negative and \"M\" for Maybe Negative.',\n",
    "    '\\nWould the last message in the conversation above be perceived as blunt/negative/offensive by its recipient? Format your output like this: \"Y\" if Yes and \"N\" if No and \"M\" if Maybe.',\n",
    "    '\\nWould the last message in the conversation above be perceived as blunt/negative/offensive by its recipient? Format your output like this: \"Y\" if Yes and \"N\" if No.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'State if the tone/intent of the last message in the conversation above is negative (even if it is slightly negative) or not. Format output like this: \"Y\" for yes and \"N\" for no.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 0\n",
    "Context: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output10/output-10-1-Y.txt\",history =1, context=True, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 264 264\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.44871794871794873\n",
      "Recall: 0.7291666666666666\n",
      "F1 Score: 0.5555555555555556\n",
      "Accuracy: 0.7878787878787878\n",
      "FP 43\n",
      "FN 13\n",
      "TP 35\n",
      "TN 173\n",
      "Total: 264\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "#reads output file\n",
    "#records labels AND indexes for all convos that were NOT skipped\n",
    "#reads angryReal40.txt\n",
    "#extracts labels for all convos that were NOT skipped by matching their indexes with the indexes collected previously\n",
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output10/output-10-0-Y.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 1\n",
    "Context: Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  And after that ? \n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  What ? He can't do this to you . \n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Person 2:  We'll see what happens after court . \n",
      "\n",
      "Total Requests: 294\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output10/output-10-1-Y.txt\",history =1, context=True, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 264 264\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.47435897435897434\n",
      "Recall: 0.7708333333333334\n",
      "F1 Score: 0.5873015873015872\n",
      "Accuracy: 0.803030303030303\n",
      "FP 41\n",
      "FN 11\n",
      "TP 37\n",
      "TN 175\n",
      "Total: 264\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "#reads output file\n",
    "#records labels AND indexes for all convos that were NOT skipped\n",
    "#reads angryReal40.txt\n",
    "#extracts labels for all convos that were NOT skipped by matching their indexes with the indexes collected previously\n",
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output10/output-10-1-Y.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 2\n",
    "Context: Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "Person 1:  What's that ? '\n",
      "Person 2:  You spend two weeks with dad in his ambulance as an EMT trainee and volunteer at the emergency room for one month . '\n",
      "Person 1:  And after that ? '\n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Total Requests: 288\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output10/output-10-2-Y.txt\",history =2, context=True, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 274 274\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.42857142857142855\n",
      "Recall: 0.78\n",
      "F1 Score: 0.5531914893617021\n",
      "Accuracy: 0.7700729927007299\n",
      "FP 52\n",
      "FN 11\n",
      "TP 39\n",
      "TN 172\n",
      "Total: 274\n",
      "Total Convos: 38\n"
     ]
    }
   ],
   "source": [
    "#reads output file\n",
    "#records labels AND indexes for all convos that were NOT skipped\n",
    "#reads angryReal40.txt\n",
    "#extracts labels for all convos that were NOT skipped by matching their indexes with the indexes collected previously\n",
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output10/output-10-2-Y.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 0\n",
    "Context: N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Total Requests: 295\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output10/output-10-0-N.txt\",history =0, context=False, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 264 264\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.4375\n",
      "Recall: 0.7291666666666666\n",
      "F1 Score: 0.546875\n",
      "Accuracy: 0.7803030303030303\n",
      "FP 45\n",
      "FN 13\n",
      "TP 35\n",
      "TN 171\n",
      "Total: 264\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "#reads output file\n",
    "#records labels AND indexes for all convos that were NOT skipped\n",
    "#reads angryReal40.txt\n",
    "#extracts labels for all convos that were NOT skipped by matching their indexes with the indexes collected previously\n",
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output10/output-10-0-N.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 1\n",
    "Context: N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  What ? \n",
      "Person 1:  Any body piercing ? Stick out your tongue ! \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  And after that ? \n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  What ? He can't do this to you . \n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Person 2:  We'll see what happens after court . \n",
      "\n",
      "Total Requests: 293\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output10/output-10-1-N.txt\",history =1, context=False, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 256 256\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.46153846153846156\n",
      "Recall: 0.7659574468085106\n",
      "F1 Score: 0.5760000000000001\n",
      "Accuracy: 0.79296875\n",
      "FP 42\n",
      "FN 11\n",
      "TP 36\n",
      "TN 167\n",
      "Total: 256\n",
      "Total Convos: 36\n"
     ]
    }
   ],
   "source": [
    "#reads output file\n",
    "#records labels AND indexes for all convos that were NOT skipped\n",
    "#reads angryReal40.txt\n",
    "#extracts labels for all convos that were NOT skipped by matching their indexes with the indexes collected previously\n",
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output10/output-10-1-N.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 2 Context: N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "Person 1:  What's that ? '\n",
      "Person 2:  You spend two weeks with dad in his ambulance as an EMT trainee and volunteer at the emergency room for one month . '\n",
      "Person 1:  And after that ? '\n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Hi , Mikel . What's with you ? You look angry . '\n",
      "Person 2:  Nah , I just check my weight , I'm getting fatter . '\n",
      "Person 1:  True , you are getting a really pot belly , aren't you ? '\n",
      "Person 2:  I'll get you for that comments , George . '\n",
      "Person 1:  Just kidding , M . Why don't you come work out with me ? '\n",
      "Person 2:  ah , I don't know a fit works . Last time , all I saw the gym were bunch of lidos , like me . '\n",
      "Person 1:  It works if you keep at it . Come on , let's go ! '\n",
      "Person 2:  All right . But so help me it better work . '\n",
      "Person 1:  This feels great . I'm all reed up . I can keep going all night . '\n",
      "\n",
      "Total Requests: 287\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output10/output-10-2-N.txt\",history =2, context=False, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 259 259\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.4523809523809524\n",
      "Recall: 0.7755102040816326\n",
      "F1 Score: 0.5714285714285715\n",
      "Accuracy: 0.7799227799227799\n",
      "FP 46\n",
      "FN 11\n",
      "TP 38\n",
      "TN 164\n",
      "Total: 259\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "#reads output file\n",
    "#records labels AND indexes for all convos that were NOT skipped\n",
    "#reads angryReal40.txt\n",
    "#extracts labels for all convos that were NOT skipped by matching their indexes with the indexes collected previously\n",
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output10/output-10-2-N.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'State if the tone/intent of the last message in the conversation above is negative (only if it is extremely negative) or not. Format output like this: \"Y\" for yes and \"N\" for no.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History:0 Context: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "K Produced As an AI, I need the conversation above to analyze and answer your request, which you didn't provide. Could you please provide the conversation to identify whether the final message in it has a negative tone/intent or not? \n",
      "Prompt: The topic of the following conversation is Ordinary Life.\n",
      "Person 1:  What about this ? State if the tone/intent of the last message in the conversation above is negative (only if it is extremely negative) or not. Format output like this: \"Y\" for yes and \"N\" for no.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Total Requests: 295\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output11/output-11-0-Y.txt\",history =0, context=True, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 264 264\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.4583333333333333\n",
      "F1 Score: 0.5432098765432098\n",
      "Accuracy: 0.8593155893536122\n",
      "FP 11\n",
      "FN 26\n",
      "TP 22\n",
      "TN 204\n",
      "Total: 263\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output11/output-11-0-Y.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 1 Context: Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  And after that ? \n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  What ? He can't do this to you . \n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Person 2:  We'll see what happens after court . \n",
      "\n",
      "Total Requests: 294\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output11/output-11-1-Y.txt\",history =1, context=True, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 264 264\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.6071428571428571\n",
      "Recall: 0.3541666666666667\n",
      "F1 Score: 0.4473684210526316\n",
      "Accuracy: 0.8409090909090909\n",
      "FP 11\n",
      "FN 31\n",
      "TP 17\n",
      "TN 205\n",
      "Total: 264\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output11/output-11-1-Y.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 2 Context Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "Person 1:  What's that ? '\n",
      "Person 2:  You spend two weeks with dad in his ambulance as an EMT trainee and volunteer at the emergency room for one month . '\n",
      "Person 1:  And after that ? '\n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Total Requests: 288\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output11/output-11-2-Y.txt\",history =2, context=True, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 274 274\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.6571428571428571\n",
      "Recall: 0.46\n",
      "F1 Score: 0.5411764705882354\n",
      "Accuracy: 0.8576642335766423\n",
      "FP 12\n",
      "FN 27\n",
      "TP 23\n",
      "TN 212\n",
      "Total: 274\n",
      "Total Convos: 38\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output11/output-11-2-Y.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 0 Context No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Total Requests: 295\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output11/output-11-0-N.txt\",history =0, context=False, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 264 264\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.4166666666666667\n",
      "F1 Score: 0.5128205128205129\n",
      "Accuracy: 0.8560606060606061\n",
      "FP 10\n",
      "FN 28\n",
      "TP 20\n",
      "TN 206\n",
      "Total: 264\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output11/output-11-0-N.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 1 Context No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 2:  What ? \n",
      "Person 1:  Any body piercing ? Stick out your tongue ! \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  And after that ? \n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  What ? He can't do this to you . \n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . \n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1:  Oh my gosh . Damn ... screw the court , I would burn down their store . \n",
      "Person 2:  We'll see what happens after court . \n",
      "\n",
      "Total Requests: 293\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output11/output-11-1-N.txt\",history =1, context=False, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 256 256\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.6896551724137931\n",
      "Recall: 0.425531914893617\n",
      "F1 Score: 0.5263157894736842\n",
      "Accuracy: 0.859375\n",
      "FP 9\n",
      "FN 27\n",
      "TP 20\n",
      "TN 200\n",
      "Total: 256\n",
      "Total Convos: 36\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output11/output-11-1-N.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History: 2 Context No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "Person 1:  What's that ? '\n",
      "Person 2:  You spend two weeks with dad in his ambulance as an EMT trainee and volunteer at the emergency room for one month . '\n",
      "Person 1:  And after that ? '\n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Hi , Mikel . What's with you ? You look angry . '\n",
      "Person 2:  Nah , I just check my weight , I'm getting fatter . '\n",
      "Person 1:  True , you are getting a really pot belly , aren't you ? '\n",
      "Person 2:  I'll get you for that comments , George . '\n",
      "Person 1:  Just kidding , M . Why don't you come work out with me ? '\n",
      "Person 2:  ah , I don't know a fit works . Last time , all I saw the gym were bunch of lidos , like me . '\n",
      "Person 1:  It works if you keep at it . Come on , let's go ! '\n",
      "Person 2:  All right . But so help me it better work . '\n",
      "Person 1:  This feels great . I'm all reed up . I can keep going all night . '\n",
      "\n",
      "Total Requests: 287\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompt,\"output11/output-11-2-N.txt\",history =2, context=False, max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths: 259 259\n",
      "\n",
      "\n",
      "**Stats**\n",
      "\n",
      "\n",
      "Precision: 0.6875\n",
      "Recall: 0.4489795918367347\n",
      "F1 Score: 0.5432098765432098\n",
      "Accuracy: 0.8571428571428571\n",
      "FP 10\n",
      "FN 27\n",
      "TP 22\n",
      "TN 200\n",
      "Total: 259\n",
      "Total Convos: 37\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels,n = processData('angryReal40.txt', 'output11/output-11-2-N.txt')\n",
    "print(\"Lengths:\",len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "print(\"\\n\\n**Stats**\\n\\n\")\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt The topic of the following conversation is Ordinary Life.\n",
      "Person 1: So Dick , how about getting some coffee for tonight ? State if the tone/intent of the following message in the conversation above is negative (even if it is slightly negative) or not: \n",
      " Person 1: So Dick , how about getting some coffee for tonight ?  \n",
      " Format output like this: \"Y\" for yes and \"N\" for no.\n",
      "Final Prompt The topic of the following conversation is Ordinary Life.\n",
      "Person 2:  Coffee ? I don ’ t honestly like that kind of stuff . State if the tone/intent of the following message in the conversation above is negative (even if it is slightly negative) or not: \n",
      " Person 2:  Coffee ? I don ’ t honestly like that kind of stuff .  \n",
      " Format output like this: \"Y\" for yes and \"N\" for no.\n",
      "Final Prompt The topic of the following conversation is Ordinary Life.\n",
      "Person 1:  Come on , you can at least try a little , besides your cigarette . State if the tone/intent of the following message in the conversation above is negative (even if it is slightly negative) or not: \n",
      " Person 1:  Come on , you can at least try a little , besides your cigarette .  \n",
      " Format output like this: \"Y\" for yes and \"N\" for no.\n",
      "Final Prompt The topic of the following conversation is Ordinary Life.\n",
      "Person 2:  What ’ s wrong with that ? Cigarette is the thing I go crazy for . State if the tone/intent of the following message in the conversation above is negative (even if it is slightly negative) or not: \n",
      " Person 2:  What ’ s wrong with that ? Cigarette is the thing I go crazy for .  \n",
      " Format output like this: \"Y\" for yes and \"N\" for no.\n",
      "Final Prompt The topic of the following conversation is Ordinary Life.\n",
      "Person 1:  Not for me , Dick . State if the tone/intent of the following message in the conversation above is negative (even if it is slightly negative) or not: \n",
      " Person 1:  Not for me , Dick .  \n",
      " Format output like this: \"Y\" for yes and \"N\" for no.\n",
      "Total Requests: 5\n",
      "[0]\n",
      "00010\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[570], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(angryOnlyRealLabels), \u001b[39mlen\u001b[39m(angryOnlyGenLabels))\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIssue in: \u001b[39m\u001b[39m\"\u001b[39m, fileName)\n\u001b[0;32m---> 26\u001b[0m PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n)\n",
      "Cell \u001b[0;32mIn[501], line 41\u001b[0m, in \u001b[0;36mPRFA\u001b[0;34m(angryOnlyRealLabels, angryOnlyGenLabels, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m precision \u001b[39m=\u001b[39m TP \u001b[39m/\u001b[39m (TP \u001b[39m+\u001b[39m FP)\n\u001b[1;32m     40\u001b[0m recall \u001b[39m=\u001b[39m TP \u001b[39m/\u001b[39m (TP \u001b[39m+\u001b[39m FN)\n\u001b[0;32m---> 41\u001b[0m f1_score \u001b[39m=\u001b[39m \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m (precision \u001b[39m*\u001b[39;49m recall) \u001b[39m/\u001b[39;49m (precision \u001b[39m+\u001b[39;49m recall)\n\u001b[1;32m     42\u001b[0m accuracy \u001b[39m=\u001b[39m (TP \u001b[39m+\u001b[39m TN) \u001b[39m/\u001b[39m (TP \u001b[39m+\u001b[39m TN \u001b[39m+\u001b[39m FP \u001b[39m+\u001b[39m FN)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrecision:\u001b[39m\u001b[39m\"\u001b[39m, precision)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "historyOptions=[0,1,2]\n",
    "contextOptions=[True,False]\n",
    "promptNo=[\"20\"]\n",
    "promptOptions=[\n",
    "    'State if the tone/intent of the following message in the conversation above is negative (even if it is slightly negative) or not: \\n @rukhshan@ \\n Format output like this: \"Y\" for yes and \"N\" for no.'\n",
    "    ]\n",
    "groundPath = \"/Users/rukhshanharoon/Desktop/LLMChattingApp/Analysis/angryReal40.txt\"\n",
    "maxCon=40\n",
    "pathFinalOutput=\"/Users/rukhshanharoon/Desktop/LLMChattingApp/Analysis/finalOutput.csv\"\n",
    "\n",
    "if(len(promptNo)!=len(promptOptions)):\n",
    "    raise ValueError(\"promptNo not equal to promptOptions!\")\n",
    "\n",
    "for promptIndex in range(len(promptOptions)):\n",
    "    for historySelected in historyOptions:\n",
    "        for contextSelected in contextOptions:\n",
    "            folderName = \"/Users/rukhshanharoon/Desktop/LLMChattingApp/Analysis/output\"+promptNo[promptIndex]\n",
    "            if not os.path.exists(folderName):\n",
    "                os.makedirs(folderName)\n",
    "            fileName = \"output-\"+promptNo[promptIndex]+\"-\"+str(historySelected)+\"-\"+ ('Y.txt' if contextSelected else 'N.txt')\n",
    "            genPath = folderName + \"/\" + fileName\n",
    "            labelsGPT = readSpecificConversations(textFile,angryConversations,promptOptions[promptIndex],genPath,historySelected, contextSelected, maxCon)\n",
    "            angryOnlyRealLabels, angryOnlyGenLabels,n = processData(groundPath, genPath)\n",
    "            if(len(angryOnlyRealLabels) != len(angryOnlyGenLabels)):\n",
    "                print(\"Length of ground and gen not equal!\")\n",
    "                print(len(angryOnlyRealLabels), len(angryOnlyGenLabels))\n",
    "                print(\"Issue in: \", fileName)\n",
    "                df = PRFA(angryOnlyRealLabels,angryOnlyGenLabels,n, promptNo[promptIndex],maxCon, promptOptions[promptIndex],historySelected,contextSelected, pathFinalOutput)\n",
    "            else:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation index:  15\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  95\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  97\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 5 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  155\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  178\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  222\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  300\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 8 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  306\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  306\n",
      "Skipped by GPT.\n",
      "Conversation index:  306\n",
      "Skipped by GPT.\n",
      "Conversation index:  306\n",
      "Skipped by GPT.\n",
      "Conversation index:  311\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  327\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 8 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  338\n",
      "Skipped by GPT.\n",
      "Conversation index:  338\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  338\n",
      "Skipped by GPT.\n",
      "Conversation index:  357\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  364\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  368\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  370\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  380\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  408\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  417\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  490\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  519\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  529\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  576\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  636\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  769\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  814\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  814\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  814\n",
      "Skipped by GPT.\n",
      "Conversation index:  935\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  935\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  935\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 8 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Total Requests: 283\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompts[1],\"outputSlightlyAngryGPT.txt\", max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 20, 25, 37]\n",
      "17\n",
      "20\n",
      "25\n",
      "37\n",
      "40\n",
      "251 251\n"
     ]
    }
   ],
   "source": [
    "\n",
    "angryOnlyRealLabels, angryOnlyGenLabels = processData('angryReal40.txt', 'outputSlightlyAngryGPT.txt')\n",
    "print(len(angryOnlyRealLabels),len(angryOnlyGenLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detects even slightly negative messages:\n",
      "Precision: 0.45121951219512196\n",
      "Recall: 0.7708333333333334\n",
      "F1 Score: 0.5692307692307692\n",
      "Accuracy: 0.7768924302788844\n"
     ]
    }
   ],
   "source": [
    "print(\"Detects even slightly negative messages:\") \n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation index:  15\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  95\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  97\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  155\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  178\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  222\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  300\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  306\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  311\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 6 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  327\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  338\n",
      "Skipped by GPT.\n",
      "Conversation index:  338\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  338\n",
      "Skipped by GPT.\n",
      "Conversation index:  357\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  364\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  368\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  370\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  370\n",
      "Skipped by GPT.\n",
      "Conversation index:  380\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  408\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  417\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  490\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  519\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  529\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  576\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  636\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  769\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  814\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  814\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  935\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  935\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 5 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Conversation index:  935\n",
      "Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 7 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit. END\n",
      "Total Requests: 287\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompts[2],\"outputExtremelyAngryGPT.txt\", max=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 25]\n",
      "20\n",
      "25\n",
      "40\n",
      "274 274\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels = processData('angryReal40.txt', 'outputExtremelyAngryGPT.txt')\n",
    "print(len(angryOnlyRealLabels),len(angryOnlyGenLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detects extremely negative messages only:\n",
      "Precision: 0.627906976744186\n",
      "Recall: 0.54\n",
      "F1 Score: 0.5806451612903226\n",
      "Accuracy: 0.8576642335766423\n"
     ]
    }
   ],
   "source": [
    "print(\"Detects extremely negative messages only:\") \n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDialogue(index, filePath=\"/Users/rukhshanharoon/Desktop/LLMChattingApp/Analysis/Dataset/ijcnlp_dailydialog/dialogues_text.txt\"):\n",
    "    \n",
    "    with open(filePath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        if 0 <= index <= len(lines):\n",
    "            convo = lines[index].split(\"__eou__\")\n",
    "            for utterance in convo:\n",
    "                print(utterance)\n",
    "        else:\n",
    "            print(\"Line number is out of range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation indexes with at least 1 angry emotion:\n",
      " {1, 2, 10251, 12301, 15, 32, 2084, 4132, 4134, 10281, 4143, 12337, 2100, 2101, 2102, 2106, 2111, 2113, 4161, 4165, 6214, 76, 2126, 4179, 4182, 8278, 8283, 2140, 4188, 95, 96, 97, 2148, 4196, 12395, 4205, 4208, 4211, 12403, 2168, 4216, 4217, 12408, 4220, 8318, 8321, 4227, 4228, 8324, 4231, 136, 12425, 4237, 4243, 10388, 10389, 10390, 10392, 10393, 155, 4251, 157, 10395, 10396, 4256, 4257, 4259, 4263, 6139, 2218, 10413, 12653, 2223, 2224, 4273, 178, 10415, 2228, 10416, 2230, 4278, 4280, 186, 4283, 2236, 2237, 10429, 2239, 2241, 2243, 4308, 2262, 4310, 4314, 222, 4320, 4321, 2275, 4327, 2280, 8428, 2285, 12530, 4348, 2302, 4354, 8452, 4362, 2324, 10517, 4377, 4379, 4387, 295, 296, 6440, 8487, 300, 4398, 306, 311, 8503, 4414, 10564, 2374, 327, 4423, 8521, 4426, 4427, 2380, 8522, 8526, 2383, 2384, 4431, 338, 2387, 8534, 2391, 351, 2402, 4451, 12642, 357, 364, 2413, 4460, 2415, 368, 2416, 370, 4465, 12284, 12652, 4470, 12656, 4472, 2427, 380, 4477, 12674, 2435, 12677, 12678, 12679, 392, 12680, 12681, 4491, 4495, 2451, 4500, 4502, 408, 8600, 4506, 4512, 417, 4515, 2471, 4521, 2474, 2476, 2478, 4526, 4529, 4532, 4533, 4534, 2489, 6588, 4544, 6593, 8645, 4550, 2505, 4553, 6601, 2509, 2510, 6605, 8656, 4562, 2515, 2518, 4567, 6615, 2523, 2524, 2525, 2526, 4576, 4580, 4581, 2534, 4582, 4583, 10727, 490, 4590, 4592, 4596, 8692, 4598, 6652, 4606, 519, 4615, 4616, 6666, 8715, 4621, 4624, 529, 4625, 8726, 4633, 6684, 8738, 4648, 4662, 4663, 2617, 576, 4674, 4680, 6732, 8792, 2651, 2654, 2657, 2661, 6757, 4719, 4720, 4722, 6776, 636, 2687, 4743, 8840, 8843, 10895, 8849, 4756, 8853, 4759, 4760, 4761, 4762, 10908, 8864, 4769, 4770, 2728, 4776, 4777, 4778, 4780, 4782, 4783, 4787, 4794, 10938, 4800, 4801, 2758, 4816, 13017, 8922, 4829, 2785, 8936, 8939, 2811, 2816, 769, 8963, 6922, 789, 13088, 814, 9019, 2887, 9031, 9033, 4949, 2902, 4951, 2924, 2937, 4993, 11151, 7059, 917, 935, 2992, 3006, 7105, 9154, 9158, 967, 3017, 9166, 3023, 9168, 9170, 3030, 9179, 991, 3042, 3050, 11256, 3073, 3077, 3078, 3084, 3085, 3087, 1042, 3091, 3092, 3095, 3100, 3102, 3105, 3106, 3109, 3111, 5159, 3113, 3116, 3118, 3120, 5185, 5195, 3156, 3157, 5216, 5228, 9328, 9332, 9334, 3215, 1176, 9376, 1194, 3249, 3256, 3259, 7356, 5310, 3263, 3266, 9414, 11463, 3286, 1245, 5357, 3323, 3327, 1301, 9502, 1319, 5431, 9538, 1347, 1348, 11597, 1364, 11605, 1377, 7521, 5477, 3445, 11637, 11646, 11647, 11661, 1423, 11666, 1428, 1429, 11672, 1443, 3493, 3495, 3499, 7596, 3504, 11700, 1483, 7629, 7637, 1494, 7639, 1504, 1525, 1530, 3579, 5628, 3582, 1535, 3586, 1539, 1542, 1543, 7686, 11783, 9739, 1595, 11837, 1598, 11851, 11854, 11856, 11858, 11863, 11865, 1654, 9858, 1668, 11909, 9867, 9874, 9877, 5794, 5795, 7844, 7852, 3758, 5809, 3764, 3769, 7866, 3775, 7875, 9927, 1741, 5843, 1752, 1779, 12020, 1791, 5889, 1799, 9994, 9999, 3862, 12067, 1828, 5936, 12083, 5958, 12105, 12106, 5971, 5982, 10092, 1922, 12163, 10121, 10122, 10124, 10127, 1940, 12191, 10566, 12206, 1969, 1977, 8123, 10175, 12235, 10190, 1999, 12240, 6107, 12617, 2026, 2029, 10224, 2033, 12280, 12282, 2043, 6140}\n"
     ]
    }
   ],
   "source": [
    "#conversations with at least one message labeled as angry\n",
    "angryConversations = extractEmotionConversations(emotionFile,textFile,'1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So Dick , how about getting some coffee for tonight ? \n",
      " Coffee ? I don ’ t honestly like that kind of stuff . \n",
      " Come on , you can at least try a little , besides your cigarette . \n",
      " What ’ s wrong with that ? Cigarette is the thing I go crazy for . \n",
      " Not for me , Dick . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printDialogue(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jessie , I ’ m afraid I can ’ t come back home for dinner tonight . \n",
      " Not back home for dinner again ? That ’ s the third time this week ! \n",
      " I ’ m sorry . Our company has just opened . There are always too many things to handle . You know that . \n",
      " You don ’ t have to explain . Suit yourself . \n",
      " I apologize . You have my word , I ’ ll spend some time with you on the weekend . I promise . \n",
      " We ’ ll see . \n",
      " Thank you for understanding . I promise I ’ ll make it up to you . \n",
      "\n",
      "\n",
      "Conversation indexes with at least 1 angry emotion:\n",
      " 508\n"
     ]
    }
   ],
   "source": [
    "printDialogue(95)\n",
    "print(\"Conversation indexes with at least 1 angry emotion:\\n\",len(sorted(angryConversations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mark , can you dress the twins for me while I take a shower ? '\n",
      "Person 2:  You don't know what you are asking ! '\n",
      "Person 1:  Please ! You can do it . Their clothes are all laid out on the bed . '\n",
      "Person 2:  Remember what happened last time ? '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "Person 1:  What's that ? '\n",
      "Person 2:  You spend two weeks with dad in his ambulance as an EMT trainee and volunteer at the emergency room for one month . '\n",
      "Person 1:  And after that ? '\n",
      "Person 2:  After that , we'll let you decide if you still want a murder , I mean motorcycle . '\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Total Requests: 286\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompts[3],\"outputYesNoMaybeGPT.txt\", max=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 20, 25]\n",
      "17\n",
      "20\n",
      "25\n",
      "40\n",
      "266 266\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels = processData('angryReal40.txt', 'outputYesNoMaybeGPT.txt')\n",
    "print(len(angryOnlyRealLabels),len(angryOnlyGenLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detects extremely negative messages only:\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.46875\n",
      "F1 Score: 0.6\n",
      "Accuracy: 0.9074074074074074\n"
     ]
    }
   ],
   "source": [
    "print(\"Detects extremely negative messages only:\") \n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So Dick , how about getting some coffee for tonight ? \n",
      " Coffee ? I don ’ t honestly like that kind of stuff . \n",
      " Come on , you can at least try a little , besides your cigarette . \n",
      " What ’ s wrong with that ? Cigarette is the thing I go crazy for . \n",
      " Not for me , Dick . \n",
      "\n",
      "\n",
      "Conversation indexes with at least 1 angry emotion:\n",
      " 508\n"
     ]
    }
   ],
   "source": [
    "printDialogue(sorted(angryConversations)[0])\n",
    "print(\"Conversation indexes with at least 1 angry emotion:\\n\",len(sorted(angryConversations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mark , can you dress the twins for me while I take a shower ? '\n",
      "Person 2:  You don't know what you are asking ! '\n",
      "Person 1:  Please ! You can do it . Their clothes are all laid out on the bed . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: You seem to be upset . What ’ s eating you ? '\n",
      "Person 2:  It ’ s because of my son . Dick keeps getting on my nerves '\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Total Requests: 282\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompts[4],\"outputYNMRecipientGPT.txt\", max=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 20, 25, 38]\n",
      "17\n",
      "20\n",
      "25\n",
      "38\n",
      "40\n",
      "261 261\n",
      "Precision: 0.6829268292682927\n",
      "Recall: 0.5957446808510638\n",
      "F1 Score: 0.6363636363636364\n",
      "Accuracy: 0.8773946360153256\n",
      "Out of a total  41  messages flagged by GPT,  28  or  68.29268292682927 % were actually positive. Total number of positives in the ground truth is:  47 .\n"
     ]
    }
   ],
   "source": [
    "angryOnlyRealLabels, angryOnlyGenLabels = processData('angryReal40.txt', 'outputYNMRecipientGPT.txt')\n",
    "print(len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mark , can you dress the twins for me while I take a shower ? '\n",
      "Person 2:  You don't know what you are asking ! '\n",
      "Person 1:  Please ! You can do it . Their clothes are all laid out on the bed . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: Mom , I want to get a motorcycle . Is that all right with you ? '\n",
      "Person 2:  You mean a murder cycle ? Did you know more cyclists die in accidents than motorists ? '\n",
      "Person 1:  Mom ! I'll wear a helmet and I'll drive sane . I promise . '\n",
      "Person 2:  I'll tell you what . You can get a motorcycle on one condition . '\n",
      "\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: What's up ? You sound a little down in dumps . '\n",
      "Person 2:  I quarreled with my roommate last night . He is really stubborn . '\n",
      "Person 1:  Calm down ! Shouting won ’ t help ? '\n",
      "Person 2:  He is really outrageous . '\n",
      "Person 1:  What happened ? '\n",
      "Person 2:  I went back home last night . You know tired as a dog , so I took a quick shower and went to bed . I couldn't fall asleep , because Brian was there in the living room , playing his stupid stereo so loud . I kindly told him to turn that down a little bit . He shouted at me . '\n",
      "Person 1:  What ? He can't do this to you . '\n",
      "Person 2:  Well , he did it anyway . Then he came into my room and shot all his shit blah blah blah ... you know . '\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Skipped by GPT.\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Skipped by GPT.\n",
      "\n",
      "Convo:\n",
      " Person 1: You seem to be upset . What ’ s eating you ? '\n",
      "Person 2:  It ’ s because of my son . Dick keeps getting on my nerves '\n",
      "\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Sleeping for 8.\n",
      "Total Requests: 282\n",
      "[17, 20, 25, 38]\n",
      "17\n",
      "20\n",
      "25\n",
      "38\n",
      "40\n",
      "261 261\n",
      "Precision: 0.5454545454545454\n",
      "Recall: 0.5106382978723404\n",
      "F1 Score: 0.5274725274725275\n",
      "Accuracy: 0.8352490421455939\n",
      "Out of a total  44  messages flagged by GPT,  24  or  54.54545454545455 % were actually positive. Total number of positives in the ground truth is:  47 .\n"
     ]
    }
   ],
   "source": [
    "labelsGPT = readSpecificConversations(textFile,angryConversations,prompts[5],\"outputYNRecipientGPT.txt\", max=40)\n",
    "angryOnlyRealLabels, angryOnlyGenLabels = processData('angryReal40.txt', 'outputYNRecipientGPT.txt')\n",
    "print(len(angryOnlyRealLabels),len(angryOnlyGenLabels))\n",
    "PRFA(angryOnlyRealLabels,angryOnlyGenLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
